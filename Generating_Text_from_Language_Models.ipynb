{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWJvKhtsjM6lcSoNNnC1Pe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iwatchkin/Language-modeling/blob/main/Generating_Text_from_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating text from language models.**"
      ],
      "metadata": {
        "id": "j5bbP_hcu-lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "LtGx2FNuv7MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "from typing import List, Set, Dict"
      ],
      "metadata": {
        "id": "zbBnt1CnwMls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ihQGFtRLxbua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "cVQW5LENxm4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing."
      ],
      "metadata": {
        "id": "Q3nveHHIzCPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('imdb')"
      ],
      "metadata": {
        "id": "Z98bY2sKzICP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "words_threshold = 50\n",
        "\n",
        "for sentence in tqdm(dataset['train']['text']):\n",
        "  sentences.extend([s.lower() for s in sent_tokenize(sentence) if len(s) <\n",
        "                    words_threshold])"
      ],
      "metadata": {
        "id": "-rAaXIrYzObC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "id": "cISxejsS16hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = defaultdict(int)\n",
        "\n",
        "for sentence in tqdm(sentences):\n",
        "  for word in word_tokenize(sentence):\n",
        "    words[word] += 1"
      ],
      "metadata": {
        "id": "ZpuLLKwS2ES7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(['<bos>', '<eos>', '<unk>', '<pad>'])\n",
        "freq_threshold = 250\n",
        "\n",
        "for word in tqdm(words):\n",
        "  if words[word] >= freq_threshold:\n",
        "    vocab.add(word)"
      ],
      "metadata": {
        "id": "_b8skcxB7f7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Vocab size: {len(vocab)}')"
      ],
      "metadata": {
        "id": "3yaX2SNC8Lbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2ind = {word: i for i, word in enumerate(vocab)}\n",
        "ind2word = {i: word for word, i in word2ind.items()}"
      ],
      "metadata": {
        "id": "aS6qnBAR8ckD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordDatset(Dataset):\n",
        "  def __init__(self, sentences: List[str], word2ind: Dict[str, int]):\n",
        "    self.sentences = sentences\n",
        "    self.word2ind = word2ind\n",
        "    self.bos_id = word2ind['<bos>']\n",
        "    self.eos_id = word2ind['<eos>']\n",
        "    self.unk_id = word2ind['<unk>']\n",
        "    self.pad_id = word2ind['<pad>']\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.sentences)\n",
        "\n",
        "  def __getitem__(self, index: int) -> List[int]:\n",
        "    tokenized_sentences = [self.bos_id]\n",
        "    tokenized_sentences += [self.word2ind.get(word, self.unk_id) for word in\n",
        "                            word_tokenize(self.sentences[index])]\n",
        "    tokenized_sentences += [self.eos_id]\n",
        "\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "ddphoJ099D90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(input_batch: List[List[int]],\n",
        "               pad_id: int = word2ind['<pad>'],\n",
        "               device: str = 'cuda') -> torch.Tensor:\n",
        "  seq_lens = [len(seq) for seq in input_batch]\n",
        "  max_seq_len = max(seq_lens)\n",
        "\n",
        "  batch = []\n",
        "  for seq in input_batch:\n",
        "    batch.append(seq + [pad_id] * (max_seq_len - len(seq)))\n",
        "\n",
        "  batch = torch.LongTensor(batch).to(device)\n",
        "  new_batch = {\n",
        "      'input_ids': batch[:,:-1],\n",
        "      'target_ids': batch[:, 1:]}\n",
        "\n",
        "  return new_batch"
      ],
      "metadata": {
        "id": "vd9hd_BTB3JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, eval_sentences = train_test_split(sentences, train_size=0.8)\n",
        "\n",
        "train_dataset = WordDatset(train_sentences, word2ind)\n",
        "eval_dataset = WordDatset(eval_sentences, word2ind)"
      ],
      "metadata": {
        "id": "TahDA9yWGER7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              collate_fn=collate_fn,\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset,\n",
        "                              collate_fn=collate_fn,\n",
        "                              batch_size=batch_size)"
      ],
      "metadata": {
        "id": "J8_5VSn-Gq0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The architecture of the language model."
      ],
      "metadata": {
        "id": "78_6gh-SHJj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, hidden_dim: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "    self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.linear_output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input_batch) -> torch.Tensor:\n",
        "    embeddings = self.embedding(input_batch)\n",
        "    output, _ = self.lstm(embeddings)\n",
        "    output = self.dropout(self.linear(self.relu(output)))\n",
        "    output = self.linear_output(self.relu(output))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "YjERZhyfHNF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The training loop of the model."
      ],
      "metadata": {
        "id": "bTGt0PZvMewg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model,criterion, eval_dataloader) -> float:\n",
        "  model.eval()\n",
        "  perplexity = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in eval_dataloader:\n",
        "      logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n",
        "      target_logits = batch['target_ids'].flatten()\n",
        "      loss = criterion(logits, target_logits)\n",
        "      perplexity.append(torch.exp(loss).item())\n",
        "\n",
        "  perplexity = sum(perplexity) / len(perplexity)\n",
        "\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "8kZrPmQoNrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "4YDA0Qx8MjJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LanguageModel(hidden_dim, vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\n",
        "optimizer = torch.optim.Adam(params=lm.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "drR2GkkFMuzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_loss = []\n",
        "perplexities = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = []\n",
        "  lm.train()\n",
        "  for batch in tqdm(train_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = lm(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n",
        "    target_logits = batch['target_ids'].flatten()\n",
        "    loss = criterion(logits, target_logits)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss.append(loss.item())\n",
        "\n",
        "  avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
        "  print(f'Epoch {epoch}: average error per epoch = {avg_loss:.3f}')\n",
        "  train_loss.append(avg_loss)\n",
        "  perplexities.append(calculate_perplexity(lm, criterion, eval_dataloader))"
      ],
      "metadata": {
        "id": "pQmCalkyNo5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss)), train_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.title('Cross Entropy Loss')"
      ],
      "metadata": {
        "id": "KTKIybQxTpOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(perplexities)), perplexities)\n",
        "plt.xlabel('epoch')\n",
        "plt.title('Perplexities')"
      ],
      "metadata": {
        "id": "rW4UEsL1UBpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation."
      ],
      "metadata": {
        "id": "nMjG1qppfwFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(model,\n",
        "                      source_sequence: str,\n",
        "                      max_num_words: int = 20) -> str:\n",
        "  device = 'cpu'\n",
        "  model = model.to(device)\n",
        "  input_ids = [word2ind['<bos>']] + [word2ind.get(word, word2ind['<unk>']) for\n",
        "                                     word in word_tokenize(source_sequence)]\n",
        "  input_ids = torch.LongTensor(input_ids).to(device)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _ in range(max_num_words):\n",
        "      next_word_probabilities = model(input_ids)[-1]\n",
        "      next_word = next_word_probabilities.squeeze().argmax()\n",
        "      input_ids = torch.cat([input_ids, next_word.unsqueeze(0)])\n",
        "\n",
        "      if next_word.item() == word2ind['<eos>']:\n",
        "        break\n",
        "\n",
        "  сontinued_sequence = ' '.join([ind2word[i.item()] for i in input_ids])\n",
        "\n",
        "  return сontinued_sequence"
      ],
      "metadata": {
        "id": "F5Gj4BPdgLCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}